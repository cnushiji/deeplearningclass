import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

tab1, tab2 = st.tabs(['1.迁移学习','2.域自适应'])

with tab1:
    """1. 迁移学习是机器学习中的一种新的学习范式，旨在解决目标领域中只有少量甚至没有标记样本的富有挑战性的学习问题。"""
    """2. 它可以有效地把模型从一个场景迁移到另一个场景，从而突破传统机器学习必须有大量标记数据作为前提的要求。"""
    """3. 其它重要原因："""
    """$ \qquad $ 1) 许多应用场景数据量小，模型过拟合，无法很好地泛化到新场景；"""
    """$ \qquad $ 2) 训练集和测试集数据存在分布迁移，模型需要强鲁棒性；"""
    """$ \qquad $ 3) 个性化和定制问题"""
    """$ \qquad $ 4) 用户隐私和数据安全"""
    """- 定义: """
    """$ \qquad $ 域domain：在某一场景下进行特定学习任务的样本空间。"""
    """$ \qquad $ 源域source domain：$ 𝐷_𝑠=\left\{𝑋_𝑠,𝑌_𝑠\\right\}, 𝑛_𝑠 $个有标签数据;"""
    """$ \qquad $ 目标域target domain: $𝐷_𝑡=\left\{𝑋_𝑡,𝑌_𝑡\\right\}, 𝑛_𝑡$个有标签数据，通常$𝑛_𝑡$远小于$𝑛_𝑠$。"""
    """ 给定源域$𝐷_𝑠$和学习任务$𝑇_𝑠$，目标域$𝐷_𝑡$和学习任务$𝑇_𝑡$，迁移学习的目的是获取源域$𝐷_𝑠$和学习任务$𝑇_𝑠$中的知识以帮助提升目标域$𝐷_𝑡$的预测函数$𝑓_𝑡(\cdot)$的学习，其中"""
    st.latex(r"""D_s\ne{D_t},或者T_s\ne{T_t}""")
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片1.png', caption='图1 TL示意图')
    """- TL的分类:"""
    """$ \qquad $ 1. 根据特征空间或标签空间是否同构，可以将迁移学习分为同构迁移学习和异构迁移学习。"""
    """$ \qquad $ 2. 根据目标域中是否有标签数据将现有的迁移学习方法分为以下三类：监督TL、半监督TL和无监督TL。"""
    """- TL算法:"""
    """要设计一个迁移学习算法，我们需要考虑一下三个主要的研究问题：何时迁移，迁移什么，怎么迁移。"""
    """$ \qquad $ 何时迁移：在什么情况下应该进行迁移技术。在某些情况下，当源域和目标域彼此不相关是，强制迁移可能不会成功，甚至会损害目标域中的学习性能，称为负迁移。"""
    """$ \qquad $ 迁移什么：哪些部分的知识可以跨域或者跨任务进行迁移。"""
    """$ \qquad $ 如何迁移：迁移学习采用何种方式进行。根据对问题的不同考量有以下分类:基于样本的迁移；基于特征的迁移；基于模型的迁移；基于关系的迁移；上述每一种迁移算法对应于知识的哪一部分被视为知识迁移的载体。"""
    """- TL和其它学习范式的关系:"""
    """$ \qquad $ 一方面，迁移学习的目标即“泛化”也是机器学习的目标。它旨在获得更加通用和更加鲁棒的机器学习模型；另一方面它与其它的ML分支的不同之处在于TL旨在泛化不同任务或域之间的共性，也就是**样本“集合”的共性**，而**经典的ML分支侧重于泛化“样本”之间的共性**。"""
    """$ \qquad $ 与TL最相关的的学习范式之一是多任务学习MTL。虽然TL和MTL都旨在泛化跨任务间的共性，但是**迁移学习侧重学习目标任务**，其中一些源任务被用作辅助信息，多任务学习旨在共同学习一组目标任务以提高每个任务的泛化性能，而无需任何源任务或者辅助任务，**所有任务具有相同的重要性**。"""
    """- 基于样本的迁移学习:"""
    """$ \qquad $ 虽然源域中的有标签数据由于域差异而无法直接使用，但在重新加权或者重采样后，一部分数据能够被目标域重新使用。"""
    """$ \qquad $ 通过这种方式，权重大的源域有标签样本被视为跨域迁移的“知识”。"""
    """$ \qquad $ 基于样本的方法背后的隐含假设是源域和目标域具有许多重叠特征。"""
    """$ \qquad $ 关键问题1：如何筛选出源域中与目标域数据具有相似分布的有标签样本；"""
    """$ \qquad $ 关键问题2：如何利用这些“相似”的数据训练出一个更准确的目标域上的学习模型。"""
    """$ \qquad $ 假设：作为数据的源域样本和目标域样本有着相同或相似的特征集。同时，原任务和目标任务的输出标签需要一致。"""
    """- 两种类型:"""
    """结合域和任务的定义，在基于样本的TL中，域间或任务间的不同之处在于:"""
    """$ \qquad $ (1) 域特征的边缘分布不一样"""
    st.latex(r"""X_s=X_t, P_s^X\ne{P_t^X},P_s^{Y|X}=P_t^{Y|X}""")
    """$ \qquad $ (2) 或任务的条件概率分布不一样"""
    st.latex(r"""X_s=X_t,P_s^{Y|X}\ne{P_t^{Y|X}}""")
    """- 非归纳式迁移学习:"""
    """$ \qquad $ 1. 定义：源任务和目标任务的条件概率分布一致。"""
    st.latex(r"""X_s=X_t,P_s^X\ne{P_t^X},P_s^{Y|X}=P_t^{Y|X}""")
    """$ \qquad $ 2. 3种估算密度比的方法:"""
    """$ \qquad \qquad $ 问题核心：如何估计边缘密度比$\\beta{(x)}=(𝑃_𝑡^𝑋)/(𝑃_𝑠^𝑋) $?"""
    """$ \qquad \qquad $ 1）转化为判别问题"""
    """$ \qquad \qquad $ 2）核平均匹配"""
    """$ \qquad \qquad $ 3）函数估计"""
    """- 归纳式迁移学习:"""
    """$ \qquad $ 1. 定义：源任务和目标任务的条件概率分布不一致。"""
    st.latex(r"""X_s=X_t,P_s^{Y|X}\ne{P_t^{Y|X}}""")
    """$ \qquad \qquad $ 目标域需要有少量有标签数据"""
    """$ \qquad $ 2. 3种方法"""
    """$ \qquad \qquad $ 1）集成源损失和目标损失"""
    """$ \qquad \qquad $ 2）Boosting风格的方法"""
    """$ \qquad \qquad $ 3）样本生成的方法"""
    """- 基于特征的TL:"""
    """$ \qquad $ 基于样本的迁移学习方法假设源域数据和目标域数据有相似的特征。这样的假设可能过于严苛而无法满足，在许多真实的场景中，源域数据和目标域数据的特征往往是不重叠的。"""
    """$ \qquad $ 解决方案：在抽象的特征空间X上进行迁移，而非原始输入空间。"""
    """$ \qquad $ 同构迁移学习：$ X_s \cap{X_t}\\ne{\emptyset}且Y_s\cap{Y_t} $"""
    """$ \qquad $ 异构迁移学习：$ X_s\cap{X_t}=\emptyset $"""
    """- 同构特征迁移学习:"""
    """想法："""
    """$ \qquad $ 训练时，学习一对映射函数(𝜑_𝑠，𝜑_𝑡)将来自源域和目标域的数据映射到相同的特征空间，从而使域之间的差异性减少，然后使用映射之后的源域和目标域数据在新的特征空间上训练目标分类器。"""
    """$ \qquad $ 测试时，先将新的数据映射到新的特征空间上，然后执行训练好的目标分类器进行预测。"""
    """- 3类方法:"""
    """$ \qquad $ 1. 最小化域间差异"""
    """$ \qquad $ 将源域和目标域数据映射到同一个空间进行比较，提取公共特征。核心在于如何定义变换后空间上的度量？"""
    """$ \qquad $ 最大均值差异Maximal Mean Discrepancy(MMD)"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片2.png', caption='图2')
    """$ \qquad $ 其中$\phi$是再生希尔伯特空间(RKHS)H的核函数，$\Vert \cdot \Vert_𝐻$表示H上的范数。"""
    """$ \qquad $ 2. 学习通用特征"""
    """$ \qquad $ 3. 特征增强"""
    """- 基于模型的迁移学习:"""
    """$ \qquad $ 基于模型的迁移学习也称为基于参数的迁移学习，其假设是在模型层次上源任务和目标任务共享部分通用知识。所迁移的知识被编码到模型参数、模型先验知识、模型架构等模型层次上。"""
    """$ \qquad $ 因此基于模型的迁移学习的核心目标是明确源域模型的何种部分有助于目标域模型的学习。"""
    """$ \qquad $ 两类技术途径："""
    """$ \qquad \qquad $ （1）基于共享模型成分的知识迁移"""
    """$ \qquad \qquad $ （2）基于正则化的知识迁移"""
    """- 基于共享模型成分的知识迁移:"""
    """$ \qquad $ 1. 基于高斯过程"""
    """$ \qquad $ 2. 利用贝叶斯模型的知识迁移"""
    """$ \qquad $ 3. 利用深度模型的模型迁移:知识蒸馏Knowledge Distillation"""
    """- 基于正则化的知识迁移:"""
    """$ \qquad $ 模型中标准的正则化形式："""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片3.png', caption='图3')
    """$ \qquad $ 这个式子表明该正则化使用正则权重𝛼正则化𝜃 。其中"""
    st.latex(r"""\theta_𝑠=\theta_0+𝑣_𝑠,\theta_𝑡=\theta_0+𝑣_𝑡""")
    """$ \qquad $ 利用任务间的不变特征(模型迁移学习中被迁移的部分、源域模型中的任务无关参数)，提高目标模型的泛化性能。"""
    """$ \qquad $ 1.基于支持向量机的正则化"""
    """$ \qquad $ 2. 基于多核学习的迁移学习"""
    """$ \qquad $ 3. 深度学习中的微调方法Fine tuning"""
    """- 基于关系的迁移学习:"""
    """定义："""
    """$ \qquad $ 基于关系的迁移学习旨在构建关系域和目标关系域之间关系知识的映射。其迁移基于以下假设：源域数据之间和目标域数据之间的关系具有共同的规律。因此在某种程度上，可以基于关系特征来传递与域无关的关系知识。"""
    """$ \qquad $ 马尔科夫逻辑网络提供了表示结构关系的理想工具。"""
    """$ \qquad $ 两类技术机制："""
    """$ \qquad \qquad $ （1）一阶关系的迁移学习"""
    """$ \qquad \qquad $ （2）二阶关系的迁移学习"""

    """- 其它Topic:"""
    """$ \qquad $ 1. 异构迁移学习"""
    """$ \qquad \qquad $ 隐式语义分析"""
    """$ \qquad \qquad $ 字典学习"""
    """$ \qquad \qquad $ 流形对齐"""
    """$ \qquad \qquad $ 深度学习"""
    """$ \qquad $ 2. 对抗式迁移学习："""
    """$ \qquad \qquad $ stylegan cyclegan等"""
    """$ \qquad $ 3. 元学习、小样本学习..."""
    """$ \qquad $ 参考书：《迁移学习》，杨强等著，机械工业出版社"""

    """- Model-Agnostic learning:"""
    """$ \qquad $ 1. 集成学习"""
    """$ \qquad $ 2. 多任务学习"""
    """$ \qquad $ 3. 迁移学习"""
    """$ \qquad $ 4. 元学习"""

with tab2:
    """- 来源：我们一般都假设训练集和测试集分布一致，但在实际中，训练集和测试集其实分布会有差异，因为测试场景非可控，因此存在测试集和训练集分布有很大差异的情况，这时候训练好的模型在测试集上效果却不理想，为解决这样的问题，出现了迁移学习。比如：我们熟知的人脸识别，如果用东方人人像来训练，最后用于识别西方人，那当然性能会明显下降。"""
    """- 情景：利用一个情景（例如，分布P1）中已经学到的内容去改善另一个情景（比如分布P2）中的泛化情况，在无监督学习任务和监督学习任务之间转移表示。
直接在源域上训练得到的分类器的分类边界无法很好地区分目标域的样本。领域自适应让源域和目标域中的样本能对齐，这样模型（比如分类器模型）就能在目标域上很好地使用了。 
"""
    """- 域自适应的定义：在迁移学习中，当源域和目标域的数据分布不同，但两个任务相同时，这种特殊的迁移学习叫做域自适应。"""
    """- 域自适应的基本思想：既然源域和目标域数据分布不一样，那么就把数据都映射到一个特征空间，在特征空间中找到一个度量准则，使得源域和目标域数据的特征分布尽量接近，于是基于源域数据特征训练的判别器，就可以用到目标域数据上。"""
    """- 举例：对同一个目标检测任务，基于源域训练出了一个模型，由于源域与目标域的特征分布存在差异，那么模型可能会在源域上过拟合，导致在目标域上测试效果不好。这就可以通过域自适应的方法将源域模型迁移到目标域上。"""
    """- 迁移学习的范围更广：可以用于特征空间和类别空间不一致的情况。即对于两种不同的任务，迁移学习也可以利用实现相似的领域只是进行迁移。"""
    """- 源域与目标域区别在哪？"""
    """ 首先对于解决数据分布不同问题，一般都是通过学习一个特征转换来入手，使得在转换过后的特征空间上，源数据 和 目标数据 分布的区分度达到最小。源域与目标域区别主要体现在数据分布上，这个问题又分为三大类："""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片56.png', caption='图56')
    """ 边缘分布：数据在特征空间中的分布，初学者可以把特征空间理解为数据分布或者数据信息就好。一般给你几万张图片，它们的分布是怎样？衡量图像分布我们通过特征（例如，haar特征，梯度，颜色直方图等等）来实现，将图像特征量化成数字，分布就能看出来了。"""
    """ 条件分布：某个确定样本的分类概率分布了，例如：伯努利分布等。"""
    """- 常用的域自适应方法："""
    """ 1.样本自适应：对源数据每一个样本加权，学习一组权使得分布差异最小化，然后重新采样，从而逼近目标域的分布。"""
    """ 2.特征自适应：将源域和目标域投影到公共特征子空间，这样两者的分布相匹配，通过学习公共的特征表示，这样在公共特征空间，源域和目标域的分布就会相同。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片57.png', caption='图57')
    """ 3.模型自适应：考虑目标域的误差，对源域误差函数进行修改。假设利用上千万的数据来训练好一个模型，当我们遇到一个新的数据领域问题的时候，就不用再重新去找几千万个数据来训练，只需把原来训练好的模型迁移到新的领域，在新的领域往往只需相对较少的数据就同样可以得到很高的精度。实现的原理则是利用模型之间存在的相似性。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./pages/图片/图片58.png', caption='图58')








