import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

tab1, tab2, tab3, tab4 = st.tabs(['1.NLP理解','2.NLP语料预处理', '3.词嵌入','4.文本和语音'])

with tab1:
    """- $\\textbf{NLP为什么重要？}$"""
    """- “语言理解时人工智能领域皇冠上的明珠。——$比尔\cdot{盖茨}$”"""
    """- 在人工智能出现之前，机器智能处理结构化的数据（例如Excel里的数据）。但是网络中大部分的数据都是非结构化的，例如：文章、图片、音频、视频..."""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片60.png', caption='图60')
    """- 在非结构数据中，文本的数量是最多的，它虽然没有图片和视频占用的空间大，但是它的信息量是最大的。"""
    """- 为了能够分词和利用这些文本信息，我们就需要利用NLP技术，让机器理解这些文本信息，并加以利用。"""

    """- $\\textbf{什么是自然语言处理？}$"""
    """- 每周动物都有自己的语言，机器也是!"""
    """- 自然语言处理（NLP）就是在机器语言和人类语言之间沟通的桥梁，以实现人机交流的目的。"""
    """- 人类通过语言来交流，狗通过汪汪叫来交流。机器也有自己的交流方式，那就是数字信息。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片61.png', caption='图61')
    """- 不同语言之间是无法沟通的，比如说人类就无法听懂狗叫，甚至不同语言的人类之间都无法直接交流，需要翻译才能交流。"""
    """- 而计算机更是如此，为了让计算机之间互相交流，人们让所有计算机都遵守一些规则，计算机的这些规则就是计算机之间的语言。"""
    """- 既然不同人类语言之间可以有翻译，娜美人类和机器之间是否可以通过”翻译“的方式直接交流呢？"""
    """- NLP就是人类和机器之间沟通的桥梁！"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片62.png', caption='图62')

    """- $\\textbf{为什么是“自然语言”处理？}$"""
    """- 自然语言就是大家平时在生活中常用的表达方式，大家平时说的“讲人话”就是这个意思。"""
    """$ \qquad $ 自然语言：我背有点驼（非自然语言：我的背部呈弯曲状）。"""

    """- $\\textbf{NLP的两大核心任务} $"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片63.png', caption='图63')
    """- NLP有2个核心的任务："""
    """$ \qquad $ 1.自然语言理解-NLU|NLI"""
    """$ \qquad $ 2.自然语言生成-NLG"""
    """- 自然语言理解-NLU|NLI"""
    """$ \qquad $ 自然语言理解就是希望机器像人一样，具备正常人的语言理解能力，由于自然语言在理解上有很多难点，所以NLU是至今还远不如人类的表现。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片64.png', caption='图64')
    """$ \qquad $ 自然语言理解的5个难点："""
    """$ \qquad \qquad $ 1.语言的多样性"""
    """$ \qquad \qquad $ 2.语言的歧义性"""
    """$ \qquad \qquad $ 3.语言的鲁棒性"""
    """$ \qquad \qquad $ 4.语言的知识依赖"""
    """$ \qquad \qquad $ 5.语言的上下文"""

    """- 自然语言生成-NLG"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片65.png', caption='图65')
    """$ \qquad $ NLG是为了跨越人类和机器之间的沟通鸿沟，讲非语言格式的数据转换成人类可以理解的语言格式，如文章、报告等。"""
    """$ \qquad $ NLG的6个步骤："""
    """$ \qquad \qquad $ 1.内容确定- Content Determination"""
    """$ \qquad \qquad $ 2.文本结构- Text Structing"""
    """$ \qquad \qquad $ 3.句子聚合- Sentence Aggregation"""
    """$ \qquad \qquad $ 4.语法化- Lexicalisation"""
    """$ \qquad \qquad $ 5.参考表达式生成- Referring Expression Generation|REG"""
    """$ \qquad \qquad $ 6.语言实现- Linguistic Realisation"""

    """- $\\textbf{NLP的5个难点}$"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片66.png', caption='图66')
    """$ \qquad $ 1.语言是没有规律的，或者说规律是错综复杂的。"""
    """$ \qquad $ 2.语言是可以自由组合的，可以组合复杂的语言表达。"""
    """$ \qquad $ 3.语言是一个开放集合，我们可以任意的发明创造一些新的表达方式。"""
    """$ \qquad $ 4.语言需要联系到实践知识，有一定的知识依赖。"""
    """$ \qquad $ 5.语言的使用要基于环境和上下文。"""

    """- $ \\textbf{NLP的4个典型应用} $"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片67.png', caption='图67')
    """- 情感分析"""
    """$ \qquad $ 互联网上有大量的文本信息，这些信息想要表达的内容是五花八门的，但是他们抒发的情感是一致的：正面/积极的-负面/消极的。"""
    """$ \qquad $ 通过情感分析，可以快速了解用户的舆情情况。"""

    """- 聊天机器人"""
    """$ \qquad $ 过去的Siri、小冰这些机器人，大家使用的动力并不强，只是当作一个娱乐的方式。但是最近几年智能音箱的快速发展让大家感受到了聊天机器人的价值。"""
    """$ \qquad $ 而且随着智能家居、智能汽车的发展，聊天机器人会有更大的使用价值。"""

    """- 语音识别"""
    """$ \qquad $ 语音识别已经成为了全民级的引用，微信里可以语音转文字，汽车中使用导航可以直接说目的地，老年人使用输入法也可以直接语音而不用学习拼音。"""

    """- 机器翻译"""
    """$ \qquad $ 目前的机器翻译准确率已经很高了，大家使用Google翻译完全可以看懂文章的大意。传统的人肉翻译未来很可能会失业。"""

    """- $\\textbf{NLP的2种途径、3个核心步骤}$"""
    """- NLP可以使用传统的机器学习方法来处理，也可以使用深度学习的方法来处理。2种不同的途径也对应着不同的处理步骤。详情如下："""
    """- 方式1：传统机器学习的NLP流程"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片68.png', caption='图68')
    """$ \qquad $ 1.语料预处理"""
    """$ \qquad \qquad $ 1.中文语料预处理4个步骤"""
    """$ \qquad \qquad $ 2.英文语料预处理的6个步骤"""
    """$ \qquad $ 2.特征工程"""
    """$ \qquad \qquad $ 1.特征提取"""
    """$ \qquad \qquad $ 2.特征选择"""
    """$ \qquad $ 3.选择分类器"""

    """- 方式2：深度学习的NLP流程"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片69.png', caption='图69')
    """$ \qquad $ 1.语料预处理"""
    """$ \qquad \qquad $ 1.中文语料预处理的四个步骤"""
    """$ \qquad \qquad $ 2.英文预料预处理的6个步骤"""
    """$ \qquad $ 2.设计模型"""
    """$ \qquad $ 3.模型训练"""

    """- 英文语料预处理的6个步骤"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片70.png', caption='图70')
    """$ \qquad $ 1.分词 - Tokenization"""
    """$ \qquad $ 2.词干提取 - Stemming"""
    """$ \qquad $ 3.词形还原 - Lemmatization"""
    """$ \qquad $ 4.词性标注 - Parts of Speech"""
    """$ \qquad $ 5.命名实体识别 - NER """
    """$ \qquad $ 6.分块 - Chunking"""

    """- 中文NLP语料预处理的4个步骤"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片71.png', caption='图71')
    """$ \qquad $ 1.中文分词 - Chinese Word Segmentation"""
    """$ \qquad $ 2.词性标注 - Parts of Speech"""
    """$ \qquad $ 3.命名实体识别 - NER"""
    """$ \qquad $ 4.去除停用词"""



with tab2:
    """- $\\textbf{1.分词 - Tokenization}$"""
    """ 1.什么是分词？"""
    """ 分词是自然语言理解- NLP的重要步骤。"""
    """ 分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片72.png', caption='图72')

    """ 2.为什么要分词？"""
    """ 1.将复杂问题转化为数学问题"""
    """$ \qquad $ 机器学习之所以看上去可以解决很多复杂的问题，是因为它把这些问题都转化为了数学问题。"""
    """$ \qquad $ 而NLP也是相同的思路，文本都是一些[非结构化数据]，我们需要先将这些数据转化为[结构化数据]，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片73.png', caption='图73')

    """ 2.词是一个比较合适的粒度"""
    """$ \qquad $ 词是表达完整含义的最小单位。"""
    """$ \qquad $ 字的粒度太小，无法表达完整含义，比如”鼠“可以是”老鼠“，也可以是”鼠标“。"""
    """$ \qquad $ 而句子的粒度太大，承载的信息量多，很难复用。比如”传统方法要分词，一个重要原因是传统方法对远距离依赖的建模能力较弱。“"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片74.png', caption='图74')
    """ 3.深度学习时代，部分任务中也可以[分字]"""
    """$ \qquad $ 深度学习时代，随着数据量和算例的爆炸式增长，很多传统的方法被颠覆。"""
    """$ \qquad $ 分词一直是NLP的基础，但是现在也不一定了，比如："""
    """$ \qquad $ 不过在一些特定任务Home，分词还是必要的。如关键词提取、命名实体识别等。"""

    """- 3.中英文分词的3个典型区别"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片75.png', caption='图75')
    """ 区别1：分词方式不同，中文更难"""
    """$ \qquad $ 英文有天然的空格作为分隔符，但是中文没有。所以如何切分是一个难点，再加上中文里一词多义的情况非常多，导致很容易出现歧义。"""
    """ 区别2：英文单词有多种形态"""
    """$ \qquad $ 英文单词存在丰富的变形变换。问了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为词性还原(Lemmatization)和词干提取(Stemming)。中文则不需要"""
    """$ \qquad $ 词性还原：does,done,doing,did需要通过词性还原恢复成do。"""
    """$ \qquad $ 词干提取：cities,children,teeth这些词，需要转换问city，child,tooth这些基本形态。"""
    """区别3：中文分词需要考虑粒度问题"""
    """$ \qquad $ 例如[中国科学技术大学]就有很多种说法："""
    """$ \qquad \qquad $ 中国科学技术大学"""
    """$ \qquad \qquad $ 中国\科学技术\大学"""
    """$ \qquad \qquad $ 中国\科学\技术\大学"""
    """$ \qquad $ 粒度越大，表达的意思就越准确，但是也会导致召回比较少。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。"""

    """- 4.中文分词的3大难点"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片76.png', caption='图76')
    """ 难点1：没有统一的标准"""
    """$ \qquad $ 目前中文分词没有统一的标准，也没有公认的规范。不同的公司和组织各有各的方法和规则。"""
    """ 难点2：歧义词如何切分"""
    """$ \qquad $ 例如[乒乓球拍卖完了]就有2种分词方式表达了2种不同的含义："""
    """$ \qquad \qquad $ 乒乓球\拍卖\完了"""
    """$ \qquad \qquad $ 乒乓\球拍\卖完了"""
    """难点3：新词的识别"""
    """$ \qquad $ 信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速地识别出这些新词是一大难点。比如当年[蓝瘦香菇]大火，就需要快速识别。"""

    """- 5.3种典型的分词方法"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片77.png', caption='图77')
    """ 分词地方法大致分为3类："""
    """$ \qquad $ 1.基于词典匹配"""
    """$ \qquad \qquad $ 优点：速度快、成本低"""
    """$ \qquad \qquad $ 缺点：适应性不强，不同领域效果差异大"""
    """$ \qquad \qquad $ 基本思想是基于词典匹配，将待分词地中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照i点的词分类，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。"""

    """$ \qquad $ 2.基于统计"""
    """$ \qquad \qquad $ 优点：适应性较强"""
    """$ \qquad \qquad $ 缺点：成本较高，速度较慢"""
    """$ \qquad \qquad $ 这类目前常用的算法是HMM、CRF、SVM、深度学习等算法，比如stanfold、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登陆此的识别都具有良好的效果。"""

    """$ \qquad $ 3.基于深度学习"""
    """$ \qquad \qquad $ 优点：准确率高、适应性强"""
    """$ \qquad \qquad $ 缺点：成本高，速度慢"""
    """$ \qquad \qquad $ 例如有人员尝试使用LSTM+CRF实现分词器，其本质上是序列标注、所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可高达97.5$\%$."""
    """$ \qquad \qquad $ 常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。"""

    """- 6.中文分词工具"""
    """ 下面排名根据GitHub上的star数排名："""
    """$ \qquad $ 1.Hanlp"""
    """$ \qquad $ 2.Stanfold分词"""
    """$ \qquad $ 3.ansj分词器"""
    """$ \qquad $ 4.哈工大LTP"""
    """$ \qquad $ 5.KCWS分词器"""
    """$ \qquad $ 6.jieba"""
    """$ \qquad $ 7.IK"""
    """$ \qquad $ 8.清华大学THULAC"""
    """$ \qquad $ 9.LCTCLAS"""

    """- 7.英文分词工具"""
    """$ \qquad \qquad $ Keras"""
    """$ \qquad \qquad $ Spacy"""
    """$ \qquad \qquad $ Gensim"""
    """$ \qquad \qquad $ NLTK"""



    """- $\\textbf{2.词干提取和3.词形还原在NLP中的什么位置？}$"""

    """- 1.词干提取和词形还原在NLP中的什么位置？"""
    """ 词干提取是英文语料预处理的一个步骤（中文并不需要），而语料预处理是 NLP 的第一步，下面这张图将让大家知道词干提取在这个知识结构中的位置。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片78.png', caption='图78')
    """- 2.什么是词干提取和词形还原？"""
    """ 词干提取 – Stemming"""
    """ $ \qquad $ 词干提取是去除单词的前后缀得到词根的过程。"""
    """ $ \qquad $ 大家常见的前后词缀有“名词的复数”、“进行式”、“过去分词”…"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片79.png', caption='图79')
    """ 词形还原 – Lemmatisation"""
    """ $ \qquad $ 词形还原是基于词典，将单词的复杂形态转变成最基础的形态。"""
    """ $ \qquad $ 词形还原不是简单地将前后缀去掉，而是会根据词典将单词进行转换。比如“drove”会转换为“drive”。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片80.png', caption='图80')
    """ 为什么要做词干提取和词形还原？"""
    """ $ \qquad $ 比如当我搜索“play basketball”时，Bob is playing basketball 也符合我的要求，，但是 play 和 playing 对于计算机来说是 2 种完全不同的东西，所以我们需要将 playing 转换成 play。"""
    """ $ \qquad $ 词干提取和词形还原的目的就是将长相不同，但是含义相同的词统一起来，这样方便后续的处理和分析。"""

    """- 3.词干提取和词形还原的4个相似点"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片81.png', caption='图81')
    """ 1.目标一致。词干提取和词形还原的目标均为将词的屈折形态或派生形态简化或归并为词干（stem）或原形的基础形式，都是一种对词的不同形态的统一归并的过程。"""
    """ 2.结果部分交叉。词干提取和词形还原不是互斥关系，其结果是有部分交叉的。一部分词利用这两类方法都能达到相同的词形转换效果。如“dogs”的词干为“dog”，其原形也为“dog”。"""
    """ 3.主流实现方法类似。目前实现词干提取和词形还原的主流实现方法均是利用语言中存在的规则或利用词典映射提取词干或获得词的原形。"""
    """ 4.应用领域相似。主要应用于信息检索和文本、自然语言处理等方面，二者均是这些应用的基本步骤。"""

    """- 4.词干提取和词形还原的5个不同点"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片82.png', caption='图82')
    """ 1.在原理上，词干提取主要是采用“缩减”的方法，将词转换为词干，如将“cats”处理为“cat”，将“effective”处理为“effect”。而词形还原主要采用“转变”的方法，将词转变为其原形，如将“drove”处理为“drive”，将“driving”处理为“drive”。"""
    """ 2.在复杂性上，词干提取方法相对简单，词形还原则需要返回词的原形，需要对词形进行分析，不仅要进行词缀的转化，还要进行词性识别，区分相同词形但原形不同的词的差别。词性标注的准确率也直接影响词形还原的准确率，因此，词形还原更为复杂。"""
    """ 3.在实现方法上，虽然词干提取和词形还原实现的主流方法类似，但二者在具体实现上各有侧重。词干提取的实现方法主要利用规则变化进行词缀的去除和缩减，从而达到词的简化效果。词形还原则相对较复杂，有复杂的形态变化，单纯依据规则无法很好地完成。其更依赖于词典，进行词形变化和原形的映射，生成词典中的有效词。"""
    """ 4.在结果上，词干提取和词形还原也有部分区别。词干提取的结果可能并不是完整的、具有意义的词，而只是词的一部分，如“revival”词干提取的结果为“reviv”，“ailiner”词干提取的结果为“airlin”。而经词形还原处理后获得的结果是具有一定意义的、完整的词，一般为词典中的有效词。"""
    """ 5.在应用领域上，同样各有侧重。虽然二者均被应用于信息检索和文本处理中，但侧重不同。词干提取更多被应用于信息检索领域，如Solr、Lucene等，用于扩展检索，粒度较粗。词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达"""

    """- 5.3种主流的词干提取算法"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片83.png', caption='图83')
    """ Porter"""
    """ $ \qquad $ 这种词干算法比较旧。它是从20世纪80年代开始的，其主要关注点是删除单词的共同结尾，以便将它们解析为通用形式。它不是太复杂，它的开发停止了。"""
    """ $ \qquad $ 通常情况下，它是一个很好的起始基本词干分析器，但并不建议将它用于复杂的应用。相反，它在研究中作为一种很好的基本词干算法，可以保证重复性。与其他算法相比，它也是一种非常温和的词干算法。"""
    """ “推荐”Snowball"""
    """ $ \qquad $ 也称为 Porter2 词干算法。它几乎被普遍认为比 Porter 更好，甚至发明 Porter 的开发者也这么认为。Snowball 在 Porter 的基础上加了很多优化。Snowball 与 Porter 相比差异约为5％。"""
    """ Lancaster"""
    """ $ \qquad $ Lancaster 的算法比较激进，有时候会处理成一些比较奇怪的单词。如果在 NLTK 中使用词干分析器，则可以非常轻松地将自己的自定义规则添加到此算法中。"""

    """- 6.词形还原的实践方法"""
    """ $ \qquad $ 词形还原是基于词典的，每种语言都需要经过语义分析、词性标注来建立完整的词库，目前英文词库是很完善的。"""
    """ $ \qquad $ Python 中的 NLTK 库包含英语单词的词汇数据库。这些单词基于它们的语义关系链接在一起。链接取决于单词的含义。特别是，我们可以利用 WordNet。"""
    st.code("""
import nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("blogs"))
#Returns blogimport nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("blogs"))
#Returns blog
    """)

    """- $\\textbf{词性标注 – Part of speech}$"""
    """- 什么是词性标注？"""
    """ 词性指以词的特点作为划分词类的根据。词类是一个语言学术语，是一种语言中词的语法分类，是以语法特征（包括句法功能和形态变化）为主要依据、兼顾词汇意义对词进行划分的结果。"""
    """ 从组合和聚合关系来说，一个词类是指：在一个语言中，众多具有相同句法功能、能在同样的组合位置中出现的词，聚合在一起形成的范畴。词类是最普遍的语法的聚合。词类划分具有层次性。如汉语中，词可以分成实词和虚词，实词中又包括体词、谓词等，体词中又可以分出名词和代词等。"""
    """ 词性标注就是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程，这也是自然语言处理中一项非常重要的基础性工作，所有对于词性标注的研究已经有较长的时间，在研究者长期的研究总结中，发现汉语词性标注中面临了许多棘手的问题。"""

    """- 中文词性标注的难点"""
    """ 汉语是一种缺乏词形态变化的语言，词的类别不能像印欧语那样，直接从词的形态变化上来判别。"""
    """ 常用词兼类现象严重。《现代汉语八百词》收取的常用词中，兼类词所占的比例高达22.5%，而且发现越是常用的词，不同的用法越多。由于兼类使用程度高，兼类现象涉及汉语中大部分词类，因而造成在汉语文本中词类歧义排除的任务量巨大。"""
    """ 研究者主观原因造成的困难。语言学界在词性划分的目的、标准等问题上还存在分歧。目前还没有一个统的被广泛认可汉语词类划分标准，词类划分的粒度和标记符号都不统一。词类划分标准和标记符号集的差异，以及分词规范的含混性，给中文信息处理带来了极大的困难。"""

    """- 词性标注4种常见方法"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片84.png', caption='图84')
    """ 关于词性标注的研究比较多，这里介绍一波常见的几类方法，包括基于规则的词性标注方法、基于统计模型的词性标注方法、基于统计方法与规则方法相结合的词性标注方法、基于深度学习的词性标注方法等。"""
    """ 1.基于规则的词性标注方法"""
    """$ \qquad $ 基于规则的词性标注方法是人们提出较早的一种词性标注方法，其基本思想是按兼类词搭配关系和上下文语境建造词类消歧规则。早期的词类标注规则一般由人工构建。"""
    """$ \qquad $ 随着标注语料库规模的增大，可利用的资源也变得越来越多，这时候以人工提取规则的方法显然变得不现实，于是乎，人们提出了基于机器学习的规则自动提出方法。"""
    """ 2.基于统计模型的词性标注方法"""
    """$ \qquad $ 统计方法将词性标注看作是一个序列标注问题。其基本思想是：给定带有各自标注的词的序列，我们可以确定下一个词最可能的词性。"""
    """$ \qquad $ 现在已经有隐马尔可夫模型（HMM）、条件随机域（CRF）等统计模型了，这些模型可以使用有标记数据的大型语料库进行训练，而有标记的数据则是指其中每一个词都分配了正确的词性标注的文本。"""
    """ 3.基于统计方法与规则方法相结合的词性标注方法"""
    """$ \qquad $ 理性主义方法与经验主义相结合的处理策略一直是自然语言处理领域的专家们不断研究和探索的问题，对于词性标注问题当然也不例外。"""
    """$ \qquad $ 这类方法的主要特点在于对统计标注结果的筛选，只对那些被认为可疑的标注结果，才采用规则方法进行歧义消解，而不是对所有情况都既使用统计方法又使用规则方法。"""
    """ 4.基于深度学习的词性标注方法"""
    """$ \qquad $ 可以当作序列标注的任务来做，目前深度学习解决序列标注任务常用方法包括LSTM+CRF、BiLSTM+CRF等。"""

    """- 词性标注工具推荐"""
    """ 1.Jieba"""
    """$ \qquad $ “结巴”中文分词：做最好的 Python 中文分词组件，可以进行词性标注。"""
    """ 2.SnowNLP"""
    """$ \qquad $ SnowNLP是一个python写的类库，可以方便的处理中文文本内容。"""
    """ 3.THULAC"""
    """$ \qquad $ THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。"""
    """ 4.StanfordCoreNLP"""
    """$ \qquad $ 斯坦福NLP组的开源，支持python接口。"""
    """ 5.HanLP"""
    """$ \qquad $ HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。"""
    """ 6.NLTK"""
    """$ \qquad $ NLTK是一个高效的Python构建的平台,用来处理人类自然语言数据。"""
    """ 7.SpaCy"""
    """$ \qquad $ 工业级的自然语言处理工具，遗憾的是不支持中文。"""

    """- $\\textbf{命名实体识别 – Named-entity recognition | NER}$"""
    """- 1.什么是命名实体识别？"""
    """ 命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。简单的讲，就是识别自然文本中的实体指名称的边界和类别。"""

    """- 2.命名实体识别的发展历史"""
    """ NER一直是NLP领域中的研究热点，从早期基于词典和规则的方法，到传统机器学习的方法，到近年来基于深度学习的方法，NER研究进展的大概趋势大致如下图所示。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片85.png', caption='图85')

    """- 3.4类常见的实现方式"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片86.png', caption='图86')
    """ 1.有监督的学习方法：这一类方法需要利用大规模的已标注语料对模型进行参数训练。目前常用的模型或方法包括隐马尔可夫模型、语言模型、最大熵模型、支持向量机、决策树和条件随机场等。值得一提的是，基于条件随机场的方法是命名实体识别中最成功的方法。"""
    """ 2.半监督的学习方法：这一类方法利用标注的小数据集（种子数据）自举学习。"""
    """ 3.无监督的学习方法：这一类方法利用词汇资源（如WordNet）等进行上下文聚类。"""
    """ 4.混合方法：几种模型相结合或利用统计方法和人工总结的知识库。"""
    """ 值得一提的是，由于深度学习在自然语言的广泛应用，基于深度学习的命名实体识别方法也展现出不错的效果，此类方法基本还是把命名实体识别当做序列标注任务来做，比较经典的方法是LSTM+CRF、BiLSTM+CRF。"""


with tab3:
    """- $\\textbf{1.文本表示（Representation）}$"""
    """ 文本是一种非结构化的数据信息，是不可以直接被计算的。"""
    """ 文本表示的作用就是将这些非结构化的信息转化为结构化的信息，这样就可以针对文本信息做计算，来完成我们日常所能见到的文本分类，情感判断等任务。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片87.png', caption='图87')
    """ 文本表示的方法有很多种，下面只介绍 3 类方式："""
    """$ \qquad $ 1.独热编码 | one-hot representation"""
    """$ \qquad $ 2.整数编码"""
    """$ \qquad $ 3.词嵌入 | word embedding"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片88.png', caption='图88')

    """- $\\textbf{2.独热编码 | one-hot representation}$"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片89.png', caption='图89')
    """ 但是在实际情况中，文本中很可能出现成千上万个不同的词，这时候向量就会非常长。其中99%以上都是0。"""
    """ one-hot 的缺点如下："""
    """$ \qquad $ 1.无法表达词语之间的关系"""
    """$ \qquad $ 2.这种过于稀疏的向量，导致计算和存储的效率都不高"""

    """- $\\textbf{3.整数编码}$"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片90.png', caption='图90')
    """ 将句子里的每个词拼起来就是可以表示一句话的向量。"""
    """ 整数编码的缺点如下："""
    """$ \qquad $ 1.无法表达词语之间的关系"""
    """$ \qquad $ 2.对于模型解释而言，整数编码可能具有挑战性。"""

    """- $\\textbf{4.什么是词嵌入 | word embedding？}$"""
    """ word embedding 是文本表示的一类方法。跟 one-hot 编码和整数编码的目的一样，不过​他有更多的优点。"""
    """ 词嵌入并不特指某个具体的算法，跟上面2种方式相比，这种方法有几个明显的优势："""
    """$ \qquad $ 1.他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。"""
    """$ \qquad $ 2.语意相似的词在向量空间上也会比较相近。"""
    """$ \qquad $ 3.通用性很强，可以用在不同的任务中。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片91.png', caption='图91')

    """- $\\textbf{5.2 种主流的 word embedding 算法}$"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片92.png', caption='图92')
    """ Word2vec"""
    """$ \qquad $ 这是一种基于统计方法来获得词向量的方法，他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。"""
    """$ \qquad $ 这种算法有2种训练模式："""
    """$ \qquad \qquad $ 1.通过上下文来预测当前词"""
    """$ \qquad \qquad $ 2.通过当前词来预测上下文"""
    """ 1.什么是 Word2vec ？"""
    """$ \qquad $ Word2vec 是 Word Embedding 的方法之一。他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。"""

    """ 2.Word2vec 的 2 种训练模式"""
    """$ \qquad $ CBOW(Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)，是Word2vec 的两种训练模式。下面简单做一下解释："""
    """$ \qquad $ CBOW通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片93.png', caption='图93')

    r"""$ \qquad \qquad \qquad $ CBOW模型包含3层，即输入层、映射层和输出层。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片2.png', caption='图2')
    r"""$ \qquad \qquad \qquad $ CBOW模型中的w(t)为目标词，在已知它的上下文w(t-2),w(t-1),w(t+1),w(t+2)的前提下预测词w(t)出现的概率，即：p(w/context(w))。目标函数为："""
    st.latex("""L=\sum_{w\in{c}}{-logp(w|context(w))}""")

    """$ \qquad $ Skip-gram用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片94.png', caption='图94')

    r"""$ \qquad \qquad \qquad $ Skip-Gram模型同样包含3层，输入层，映射层和输出层。架构如图："""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片3.png', caption='图3')
    r"""$ \qquad \qquad \qquad $ Skip-Gram模型中的w(t)为输出词，在已知词w(t)的前提下预测词w(t)的上下文w(t-2),w(t-1),w(t+1),w(t+2)，条件概率为：p(context(w)/w)。目标函数为："""
    st.latex(r"""L=\sum_{w\in{c}}{-logp(context(w)|(w))}""")

    """$ \qquad $ 优化方法："""
    """$ \qquad \qquad $ 为了提高速度，Word2vec经常采用2种加速方式： """
    """$ \qquad \qquad $ 1.Negative Sample(负采样)"""
    """$ \qquad \qquad $ 2.Hierarchical Softmax"""

    """ 3.Word2vec 的优缺点"""
    """$ \qquad $ Word2vec是18年之前的产物，18年之后想要得到更好的结果，已经不使用Word Embedding的方法了，所以也不会用到Word2Vec。"""
    """$ \qquad $ 优点："""
    """$ \qquad \qquad $ 1.由于Word2vec会考虑上下文，跟之前的Embedding方法相比，效果要更好（但不如18年之后的方法）。"""
    """$ \qquad \qquad $ 2.比之前的Embedding方法维度更少，所以速度更快。"""
    """$ \qquad \qquad $ 3.通用性很强，可以用在各种NLP任务种。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片95.png', caption='图95')
    """$ \qquad $ 缺点："""
    """$ \qquad \qquad $ 1.由于词和向量是一对一的关系，所以多义词的问题无法解决。"""
    """$ \qquad \qquad $ 2.Word2vec是一种静态的方式，虽然通用性很强，但是无法针对特定任务做动态优化。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片96.png', caption='图96')

    """ GloVe"""
    """$ \qquad $ GloVe 是对 Word2vec 方法的扩展，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。"""

with tab4:
    """- NLP任务将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言。因此，首先要做的就是将语言数字化。"""
    """- 词向量是对字典D中的任意词w,指定一个固定长度的实值向量：$𝑣(𝑤)\in{𝑅^𝑚}$,$𝑣(𝑤)$就称为w的词向量，m为词向量的长度。"""
    _, col1, _ = st.columns([1, 4, 1])
    with col1:
        st.image('./chapter4_NLP/pages/图片/图片1.jpg', caption='图1 中文文本处理的一般步骤')
    """- 语音识别已经成为了全民级的引用，微信里可以语音转文字，汽车中使用导航可以直接说目的地，老年人使用输入法也可以直接语音而不用学习拼音…"""
    """- 语音识别和生成：语音识别是将输入计算机的语音符号识别转换成书面语表示。语音生成又称文语转换、语音合成，它是指将书面文本自动转换成对应的语音表征。"""


